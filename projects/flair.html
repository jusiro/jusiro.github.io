
<!DOCTYPE html>
<html lang="en-US">
  <head>

   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ELZW942XKQ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-ELZW942XKQ');
    </script>

    <meta property="og:image" content="./assets/img/projects/25_media_flair/teaser_flair.png" />
    <meta name="twitter:image" content="./assets/img/projects/25_media_flair/teaser_flair.jpg" />

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Project page of FLAIR, a vision-language foundation model for fundus image analysis.">

    <title>FLAIR</title>

    <link rel="icon" media="(prefers-color-scheme:dark)" href="../assets/img/projects/25_media_flair/sample_macular_hole.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="../assets/img/projects/25_media_flair/sample_macular_hole.png" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>
    <link href="lib/font-awesome/css/font-awesome.min.css" type="text/css" rel="stylesheet">
        <link href="lib/normalize.css" type="text/css" rel="stylesheet">
        <script src="http://use.typekit.net/ulc1wme.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"  />

    <link rel="stylesheet" href="../assets/css/style_project.css">

    <script src="../assets/js/github-stars.js"></script>
    <script type="text/javascript" src="../assets/js/jquery.js"></script>

  </head>
  <body>

    <div class="wrapper">

      <section>

      <h2 class="project-name" style="font-weight:normal; font-size: 200%;" align="center">A Foundation LAnguage-Image model of the Retina (FLAIR)</h2>
      <h2 class="project-name" style="font-weight:normal; font-size: 200%;" align="center">Encoding expert knowledge in text supervision</h2>
      <h3 style="font-weight:normal" align="center">
          <a href="https://scholar.google.es/citations?user=1UMYgHMAAAAJ&hl" target="_blank">Julio Silva-Rodríguez</a><sup>1</sup> &#183;
          <a href="https://scholar.google.ca/citations?user=0Njg-cQAAAAJ&hl" target="_blank">Hadi Chakor</a><sup>2</sup> &#183;
          <a href="https://ca.linkedin.com/in/riadh-kobbi" target="_blank">Riadh Kobbi</a><sup>2</sup> &#183;
          <a href="https://scholar.google.es/citations?user=yHQIFFMAAAAJ&hl" target="_blank">Jose Dolz</a><sup>1</sup> &#183;
          <a href="https://scholar.google.es/citations?user=29vyUccAAAAJ&hl" target="_blank">Ismail Ben Ayed</a><sup>1</sup>
      </h3>
      <h3 style="font-weight:normal;" align="center"><sup>1</sup> ÉTS Montréal &nbsp; - &nbsp; <sup>2</sup> DIAGNOS  Inc. &nbsp; <font color=#000000></font></h3>

      <h3 style="font-weight:normal; font-size: 125%;" align="center">Medical Image Analysis 2025  &nbsp; - &nbsp;
          <a href="https://arxiv.org/pdf/2308.07898.pdf" role="button"><i class="fas fa-file-pdf"></i> Paper</a>  &nbsp; - &nbsp;
          <a href="https://github.com/jusiro/FLAIR/" role="button"><i class="fab fa-github"></i> Code</a>  &nbsp; - &nbsp;
          <a href="https://colab.research.google.com/drive/1LE50MQmsEQxMM-qvytXGeJ9WAu09w1MR?usp=sharing" role="button"><i class="fas fa-chalkboard"></i> Tutorial</a>
      </h3>

<center>
<img src="../assets/img/projects/25_media_flair/flair.png" style="background-color:white" width="800" class="figure-img img-responsive center-block">
</center>
<br>

<h4 id="contributions">Highlights</h4>
    <div style="text-align: justify ">
        <ul>
            <li>FLAIR is a <strong>large-scale vision-language foundation model</strong> for fundus image analysis.</li>
            <li>The model is pre-trained from a collection of <strong>38 open-access datasets, including 101 different ocular conditions</strong>.</li>
            <li>Encoding <strong>expert knowledge in text descriptions</strong> (e.g. "mild diabetic retinopathy" is defined by the presence
                of "few microaneurysms") boosts the performance of text-driven pre-training and inference, and allows
                exploiting categorically-labelled datasets.</li>
            <li>FLAIR shows excellent properties for <strong>zero-shot generalization to unseen categories</strong>, and <strong>efficient
                transferability</strong> trough Linear Probing in the low-data (i.e. few-shot) regime.</li>
        </ul>
    </div>

<h2>Abstract</h2>
<hr>
    <div style="text-align: justify ">

        Foundation vision-language models are currently transforming computer vision,
        and are on the rise in medical imaging fueled by their very promising generalization
        capabilities. However, the initial attempts to transfer this new paradigm to medical
        imaging have shown less impressive performances than those observed in other domains,
        due to the significant domain shift and the complex, expert domain knowledge inherent to
        medical-imaging tasks. Motivated by the need for domain-expert foundation models,
        we present FLAIR, a pre-trained vision-language model for universal retinal fundus
        image understanding. To this end, we compiled 38 open-access, mostly categorical
        fundus imaging datasets from various sources, with up to 101 different target conditions
        and 288,307 images. We integrate the expert's domain knowledge in the form of descriptive
        textual prompts, during both pre-training and zero-shot inference, enhancing the less-informative
        categorical supervision of the data. Such a textual expert's knowledge, which we compiled from
        the relevant clinical literature and community standards, describes the fine-grained features of
        the pathologies as well as the hierarchies and dependencies between them. We report comprehensive
        evaluations, which illustrate the benefit of integrating expert knowledge and the strong generalization
        capabilities of FLAIR under difficult scenarios with domain shifts or unseen categories. When adapted
        with a lightweight linear probe, FLAIR outperforms fully-trained, dataset-focused models, more so in the
        few-shot regimes. Interestingly, FLAIR outperforms by a wide margin larger-scale generalist
        image-language models and retina domain-specific self-supervised networks, which emphasizes the potential
        of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.

    </div>
    <br>

<h2 id="performance">Performance</h2>
<hr>

    <center>
    <img src="../assets/img/projects/25_media_flair/performance_flair_teaser.svg" width="900" class="figure-img img-responsive center-block">
    </center>
    <br>

<h2 id="citation">Citation</h2>
<hr>

    <p>Please cite our paper if it is helpful to your work:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{FLAIR,
    title = {A Foundation Language-Image Model of the Retina (FLAIR): encoding expert knowledge in text supervision},
    author = {Julio Silva-Rodríguez and Hadi Chakor and Riadh Kobbi and Jose Dolz and Ismail {Ben Ayed}},
    journal = {Medical Image Analysis},
    volume = {99},
    pages = {103357},
    year = {2025},
    issn = {1361-8415}
}</code></pre></div></div>

<h2 id="contact">Contact</h2>
<hr>

    Please feel free to contact us: <a href="mailto:julio-jose.silva-rodriguez@etsmtl.ca" target="_blank">julio-jose.silva-rodriguez@etsmtl.ca</a>.
    <br>
    <br>




      </section>
    </div>
    <!--
    <script src="/assets/js/scale.fix.js"></script>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-111540567-4', 'auto');
      ga('send', 'pageview');
    </script>

    -->
  </body>
</html>