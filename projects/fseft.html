
<!DOCTYPE html>
<html lang="en-US">
  <head>

   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ELZW942XKQ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-ELZW942XKQ');
    </script>

    <meta property="og:image" content="./assets/img/projects/25_media_fseft/overview_model.svg" />
    <meta name="twitter:image" content="./assets/img/projects/25_media_fseft/overview_model.svg" />

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Project page of the paper 'Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation'.">

    <title>FSEFT</title>

    <link rel="icon" media="(prefers-color-scheme:dark)" href="../assets/img/projects/25_media_fseft/example.svg" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="../assets/img/projects/25_media_fseft/example.svg" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>
    <link href="lib/font-awesome/css/font-awesome.min.css" type="text/css" rel="stylesheet">
        <link href="lib/normalize.css" type="text/css" rel="stylesheet">
        <script src="http://use.typekit.net/ulc1wme.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"  />

    <link rel="stylesheet" href="../assets/css/style_project.css">

    <script src="../assets/js/github-stars.js"></script>
    <script type="text/javascript" src="../assets/js/jquery.js"></script>

  </head>
  <body>

    <div class="wrapper">

      <section>

      <h2 class="project-name" style="font-weight:normal; font-size: 200%;" align="center">Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for
Volumetric Organ Segmentation</h2>
      <h3 style="font-weight:normal" align="center">
          <a href="https://scholar.google.es/citations?user=1UMYgHMAAAAJ&hl" target="_blank">Julio Silva-Rodr√≠guez</a> &#183;
          <a href="https://scholar.google.es/citations?user=yHQIFFMAAAAJ&hl" target="_blank">Jose Dolz</a> &#183;
          <a href="https://scholar.google.es/citations?user=29vyUccAAAAJ&hl" target="_blank">Ismail Ben Ayed</a> - √âTS Montr√©al.
      </h3>
      <h3 style="font-weight:normal; font-size: 125%;" align="center">Medical Image Analysis 2025  &nbsp; - &nbsp;
          <a href="https://arxiv.org/abs/2303.17051v4" role="button"><i class="fas fa-file-pdf"></i> Paper</a>  &nbsp; - &nbsp;
          <a href="https://github.com/jusiro/fewshot-finetuning" role="button"><i class="fab fa-github"></i> Code</a>
      </h3>
     <h2 style="font-weight:normal; font-size: 100%;" align="center"> üèÖ <span style="color:red"> Best Paper Award at 1st MICCAI Workshop on Foundation Models (MedAGI'23) </span> </h2>
<center>
<img src="../assets/img/projects/25_media_fseft/overview_model.svg" style="background-color:white" width="800" class="figure-img img-responsive center-block">
</center>
<br>

<h2 id="contributions">Highlights</h2>
    <div style="text-align: justify ">
        <ul>
            <li><b>A foundation model for volumetric organ segmentation is  released</b>, trained on nine publicly available datasets gathering 2,042 CT scans
                and 29 annotated structures via supervised pre-training.</li>
            <li>We formalize <b>Few-Shot Efficient Fine-Tuning (FSEFT), a  novel and realistic setting for transferring supervised pretrained 3D models in challenging
                clinical scenarios</b>. FSEFT considers the scarcity of adaptation supervision, using only <b>a handful of labeled samples</b> in the target task, and the
                <b>parameter efficiency</b>.</li>
             <li>Comprehensive transferability experiments point out to the potential of foundation models in low data regimes:</li>
            <ul>
                <li> For segmenting <u>known categories</u> (available during pre-training), use Black-box Adapters for low-resource adaptation.
                <li> For <u>new structures</u>, PEFT combined with supervised pre-training offers impressive performance gains.
            </ul>
        </ul>
    </div>

<br>
<h2 id="base_categories">Towards transfer learning with commodity resources.</h2>
<hr>
    <div style="text-align: justify ">
    Volumetric organ segmentation addresses a set of biologically finite target objectives. With progressive advancements on open-access data gathering and annotation,
        future foundation models will be pre-trained on an increasing number of annotated concepts. A natural transfer learning scenario to exploit foundation models
        arises when domain drifts exist in the target institution acquisition systems and demographics, but the pre-trained model has been trained to segment
        such organs. <br> In this work, we advocate for leveraging <b><u>spatial Adapters, a black-box strategy that operates over pre-computed features</u></b> - without explicit access to internal
        network weights. Such Adapters are indeed a competitive fine-tuning strategy in low data regimes, which <b><u>allows model adaptation using comodity resources, which are standard
        in clinical institutions</u></b>.

    </div>
    <br>

    <center>
    <img src="../assets/img/projects/25_media_fseft/summary_base_categories.svg" width="800" class="figure-img img-responsive center-block">
    </center>
    <br>

<br>
<h2 id="new_categories">Are foundation models useful for segmenting novel structures?</h2>
<hr>
    <div style="text-align: justify ">
    A desirable quality of transfer learning is to leverage the learned universal representations in new concepts. In particular, we argue that such a property
        is of interest if the foundation model requires a small number of examples for adaptation. For volumetric segmentation, we find a need to update the
        network decoder, which burdens the parameter-efficient adaptation. <br> We discover that <b><u>combining PEFT (LoRA, in particular), with decoder fine-tuning of
        supervised pre-trained networks offers strong transferability in the low data regime</u></b>. This is specially the case when compared to popular self-supervised
        pre-training objectives.
    </div>
    <br>

    <center>
    <img src="../assets/img/projects/25_media_fseft/summary_novel_categories.svg" width="800" class="figure-img img-responsive center-block">
    </center>

<h2 id="citation">Citation</h2>
<hr>

    <p>Please cite our paper if it is helpful to your work:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{FSEFT,
  title={Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation},
  author={Julio Silva-Rodr√≠guez and Jose Dolz and Ismail {Ben Ayed}},
  journal={Medical Image Analysis},
  year={2025}
}</code></pre></div></div>

<br>
<h2 id="contact">Contact</h2>
<hr>

    Please feel free to contact us: <a href="mailto:julio-jose.silva-rodriguez@etsmtl.ca" target="_blank">julio-jose.silva-rodriguez@etsmtl.ca</a>.
    <br>
    <br>




      </section>
    </div>
    <!--
    <script src="/assets/js/scale.fix.js"></script>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-111540567-4', 'auto');
      ga('send', 'pageview');
    </script>

    -->
  </body>
</html>