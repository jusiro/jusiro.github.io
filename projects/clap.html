
<!DOCTYPE html>
<html lang="en-US">
  <head>

   <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-ELZW942XKQ"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-ELZW942XKQ');
    </script>

    <meta property="og:image" content="./assets/img/teaser_flair.png" />
    <meta name="twitter:image" content="./assets/img/teaser_flair.jpg" />

    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="description" content="Project page of the paper 'A Closer Look at the Few-Shot Adaptation of Large
    Vision-Language Models'. Accepted for CVPR 2024.">

    <title>CLAP</title>

    <link rel="icon" media="(prefers-color-scheme:dark)" href="../assets/img/clap.png" type="image/png" />
    <link rel="icon" media="(prefers-color-scheme:light)" href="../assets/img/clap.png" type="image/png" />
    <script src="./assets/js/favicon-switcher.js" type="application/javascript"></script>
    <link href="lib/font-awesome/css/font-awesome.min.css" type="text/css" rel="stylesheet">
        <link href="lib/normalize.css" type="text/css" rel="stylesheet">
        <script src="http://use.typekit.net/ulc1wme.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css"  />

    <link rel="stylesheet" href="../assets/css/style_project.css">

    <script src="../assets/js/github-stars.js"></script>
    <script type="text/javascript" src="../assets/js/jquery.js"></script>

  </head>
  <body>

    <div class="wrapper">

      <section>

      <h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center">A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models</h2>
      <h3 style="font-weight:normal" align="center">
          <a href="https://scholar.google.es/citations?user=1UMYgHMAAAAJ&hl" target="_blank">Julio Silva-Rodríguez</a> -&nbsp;
          <a href="https://scholar.google.com/citations?user=C5k-mOYAAAAJ&hl" target="_blank">Sina Hajirimi</a> -&nbsp;
          <a href="https://scholar.google.es/citations?user=29vyUccAAAAJ&hl" target="_blank">Ismail Ben Ayed</a> -&nbsp;
          <a href="https://scholar.google.es/citations?user=yHQIFFMAAAAJ&hl" target="_blank">Jose Dolz</a>
      </h3>
      <h4 style="font-weight:normal;" align="center"><a href="https://liviamtl.ca/" target="_blank"><autocolor>ÉTS Montreal</autocolor></a> <font color=#000000></font></h4>
      <h3 style="font-weight:normal; font-size: 167%;" align="center">CVPR 2024</h3>

      <div class="cta">
          <a href="https://arxiv.org/pdf/2312.12730.pdf" role="button"><i class="fas fa-file-pdf"></i> Paper</a>
          <a href="https://github.com/jusiro/CLAP/" role="button"><i class="fab fa-github"></i> Code</a>
      </div>

<h2 id="contributions">Highlights</h2>
    <div style="text-align: justify ">
        <ul>
            <li><strong>Adapter-style efficient transfer learning</strong> allow black-box, and fast few-shot transferability of VLMs.</li>
            <li>Existing Adapters learn a <strong>combination of zero-shot prototypes and support embeddings</strong>  to leverage taks-specific predictions.</li>
            <li><strong>Pitfalls</strong>: prior Adapters require a <strong>validation subset to fix key hyperparameters</strong>, <strong>unrealistic</strong> on the few-shot data regime.</li>
            <li><strong>Proposed:</strong> Few-shot adapters with <strong>model selection strategy</strong> based only on the support set.</li>
                <ul>
                    <li><strong>Zero-shot Linear Probe (ZS-LP)</strong>: a surprisingly strong well-initialized Linear Probe.</li>
                    <li><strong>Class-Adaptive Linear Probe (CLAP)</strong>: constraining the learnt prototypes to remain close to zero-shot weights.</li>
                </ul>
        </ul>
    </div>

<br>
<h2 id="etl">Few-shot VLMs Adaptation</h2>
<hr>
    <div style="text-align: justify ">
        The adaptation of Vision-Language Models using few-shots as supervision benefits from the efficient transfer of
        the pre-trained features. Two alternatives are currently popularized: Prompt Learning, and Adapters.
    </div>

    <center>
    <img src="../assets/img/etl.svg" width="1000" class="figure-img img-responsive center-block">
    </center>
    <br>

<br>
<h2 id="pitfalls">Pitfalls on Existing Adapters</h2>
<hr>
    <div style="text-align: justify ">
        Existing Adapters exhibit strong performance only in narrowly-defined experimental setups, and with a careful
        adjustment of hyperparameters based on a large corpus of labeled samples. To outperform a carefully designed
        Linear Probing (ZS-LP) baseline, these methods require to optimize their hyperparameters on each target task,
        which is unrealistic.
    </div>
    <center>
    <img src="../assets/img/pitfalls_crosshift.svg" width="1000" class="figure-img img-responsive center-block">
    </center>
    <br>

<br>
<h2 id="proposed">Class-Adaptive Linear Probing (CLAP)</h2>
<hr>

    <div style="text-align: justify ">
        We propose a novel approach that meets the requirements of real-world scenarios. We introduce a CLass-Adaptive
        linear Probe (CLAP) objective, that constraints the learned prototypes to retain prior zero-shot knowledge
        adaptely based only on the few support shots, and uses an homogeneus learning configuration accross tasks.
    </div>

    <center>
    <img src="../assets/img/clap_results.svg" width="1500" class="figure-img img-responsive center-block">
    </center>
    <br>

<br>
<h2 id="citation">Citation</h2>
<hr>

    <p>Please cite our paper if it is helpful to your work:</p>

    <div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{clap24,
    title={A Closer Look at the Few-Shot Adaptation of Large Vision-Language Models},
    author={Julio Silva-Rodr\'iguez and Sina Hajimiri and Jose Dolz and Ismail Ben Ayed},
    booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024}
    }</code></pre></div></div>

<br>
<h2 id="contact">Contact</h2>
<hr>

    Please feel free to contact us: <a href="mailto:julio-jose.silva-rodriguez@etsmtl.ca" target="_blank">julio-jose.silva-rodriguez@etsmtl.ca</a>.
    <br>
    <br>




      </section>
    </div>
    <!--
    <script src="/assets/js/scale.fix.js"></script>

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-111540567-4', 'auto');
      ga('send', 'pageview');
    </script>

    -->
  </body>
</html>